# -*- coding: utf-8 -*-
"""Object_detection_(Masked_Datset).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lWSEKq3tBJ1gUpkDVd97l9bYMmaak4pg
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

cd/content/sample_data

Input = keras.Input(shape = (800,800,3),name = 'input img')
x = layers.Conv2D(64,3,padding ='same',activation = 'relu')(Input)
x = layers.Conv2D(64,2,padding = 'same', activation ='relu')(x)
output1 = layers.MaxPool2D(pool_size =(2,2),strides =(2,2),)(x)

input2 = layers.Conv2D(128,3,padding ='same', activation ='relu')(output1)
x = layers.Conv2D(128,3,padding ='same',activation ='relu')(input2)
output2 = layers.MaxPool2D(pool_size = (2,2),strides =(2,2))(x)

input3 = layers.Conv2D(256,3,padding ='same', activation ='relu')(output2)
x = layers.Conv2D(256,3,padding ='same', activation ='relu')(input2)
x = layers.Conv2D(256,3,padding ='same', activation ='relu')(x)
output3 = layers.MaxPool2D(pool_size = (2,2),strides =(2,2))(x)

input4 = layers.Conv2D(512,3,padding = 'same', activation = 'relu')(output3)
x = layers.Conv2D(512,3,padding = 'same', activation = 'relu')(input4)
x = layers.Conv2D(512,3,padding = 'same', activation = 'relu')(x)
output4 = layers.MaxPool2D(pool_size = (2,2),strides =(2,2))(x)

input5 = layers.Conv2D(512,3,padding ='same', activation = 'relu')(output4)
x = layers.Conv2D(512,3,padding = 'same', activation = 'relu')(input5)
x = layers.Conv2D(512,3,padding = 'same', activation = 'relu')(x)
x = layers.MaxPool2D(pool_size = (2,2),strides =(2,2))(x)
output5 = layers.Flatten()(x)
x = layers.Conv2D(1,kernel_size = 1,padding = 'same', activation = 'relu')(x)
#dense1 = layers.Dense(256,activation = 'relu')(output5)
#dense2 = layers.Dense(128,activation =  "relu")(dense1)
#dense_output = layers.Conv2D(1,(1,1),name = 'out')(x)

model = keras.Model(Input,x )

###########new_model##########

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt
import random



import pathlib
import shutil

data_dir_path = "data/"
shutil.unpack_archive(data_dir_path + "/archive.zip", data_dir_path)

data_dir = pathlib.Path(data_dir_path + "/obj/").with_suffix('')

image_files = list(data_dir.glob('*.jpg'))
print(len(image_files))

import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt
import random
import os

sample_img_path = str(image_files[777])
print(sample_img_path)
sample_img = cv.imread(sample_img_path, cv.IMREAD_COLOR)
plt.imshow(cv.cvtColor(sample_img, cv.COLOR_BGR2RGB))
plt.axis("off")
plt.show()

sample_img.shape

def list_files(full_data_path = "data/obj/", image_ext = '.jpg', split_percentage = [70, 20]):

    files = []

    discarded = 0
    masked_instance = 0

    for r, d, f in os.walk(full_data_path):
        for file in f:
            if file.endswith(".txt"):

                # first, let's check if there is only one object
                with open(full_data_path + "/" + file, 'r') as fp:
                    lines = fp.readlines()
                    if len(lines) > 1:
                        discarded += 1
                        continue


                strip = file[0:len(file) - len(".txt")]
                # secondly, check if the paired image actually exist
                image_path = full_data_path + "/" + strip + image_ext
                if os.path.isfile(image_path):
                    # checking the class. '0' means masked, '1' for unmasked
                    if lines[0][0] == '0':
                        masked_instance += 1
                    files.append(strip)

    size = len(files)
    print(str(discarded) + " file(s) discarded")
    print(str(size) + " valid case(s)")
    print(str(masked_instance) + " are masked cases")

    random.shuffle(files)

    split_training = int(split_percentage[0] * size / 100)
    split_validation = split_training + int(split_percentage[1] * size / 100)

    return files[0:split_training], files[split_training:split_validation], files[split_validation:]

training_files, validation_files, test_files = list_files()

print(str(len(training_files)) + " training files")
print(str(len(validation_files)) + " validation files")
print(str(len(test_files)) + " test files")

print(str(len(training_files)) + " training files")
print(str(len(validation_files)) + " validation files")
print(str(len(test_files)) + " test files")

input_size = 244

def format_image(img,box):
  height,width = img.shape
  max_size = max(height,width)
  r = max_size/input_size
  new_height = int(height/r)
  new_width = int(width/r)
  new_size = (new_width,new_height)
  resized = cv.resize(img, new_size, interpolation= cv.INTER_LINEAR)
  new_image = np.zeros((input_size,input_size),dtype = np.uint8)
  new_image[0:new_height,0:new_width] = resized

  x,y,w,h = box[0],box[1],box[2],box[3]
  new_box = [int((x - 0.5*w)* width / r), int((y - 0.5*h) * height / r), int(w*width / r), int(h*height / r)]
  return new_image,new_box

temp_img = cv.imread("data/obj/1-with-mask.jpg", cv.IMREAD_GRAYSCALE)
temp_box = [0.5380952380952381 ,0.4642857142857143, 0.3638095238095238,0.5857142857142857]


temp_img_formated, box = format_image(temp_img, temp_box)

temp_color_img = cv.cvtColor(temp_img_formated, cv.COLOR_GRAY2RGB)

cv.rectangle(temp_color_img, box, (0, 255, 0), 2)

plt.imshow(temp_color_img)
plt.axis("off")
plt.show()

def data_load(files, full_data_path = "data/obj/", image_ext = ".jpg"):
    X = []
    Y = []

    for file in files:
        img = cv.imread(os.path.join(full_data_path, file + image_ext), cv.IMREAD_GRAYSCALE)

        k = 1

        with open(full_data_path + "/" + file + ".txt", 'r') as fp:
            line = fp.readlines()[0]
            if line[0] == '0':
                k = 0

            box = np.array(line[1:].split(), dtype=float)

        img, box = format_image(img, box)
        img = img.astype(float) / 255.
        box = np.asarray(box, dtype=float) / input_size
        label = np.append(box, k)

        X.append(img)
        Y.append(label)

    X = np.array(X)

    X = np.expand_dims(X, axis=3)

    X = tf.convert_to_tensor(X, dtype=tf.float32)

    Y = tf.convert_to_tensor(Y, dtype=tf.float32)

    result = tf.data.Dataset.from_tensor_slices((X, Y))

    return result

raw_train_ds = data_load(training_files)

raw_validation_ds = data_load(validation_files)

raw_test_ds = data_load(test_files)



CLASSES = 2

def format_instance(image, label):
    return image, (tf.one_hot(int(label[4]), CLASSES), [label[0], label[1], label[2], label[3]])

BATCH_SIZE = 32

# see https://www.tensorflow.org/guide/data_performance

def tune_training_ds(dataset):
    dataset = dataset.map(format_instance, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.shuffle(1024, reshuffle_each_iteration=True)
    dataset = dataset.repeat() # The dataset be repeated indefinitely.
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset

train_ds = tune_training_ds(raw_train_ds)



def tune_validation_ds(dataset):
    dataset = dataset.map(format_instance, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(len(validation_files) // 4)
    dataset = dataset.repeat()
    return dataset

validation_ds = tune_validation_ds(raw_validation_ds)

plt.figure(figsize=(20, 10))
for images, labels in train_ds.take(1):
    for i in range(BATCH_SIZE):
        ax = plt.subplot(4, BATCH_SIZE//4, i + 1)
        label = labels[0][i]
        box = (labels[1][i] * input_size)
        box = tf.cast(box, tf.int32)

        image = images[i].numpy().astype("float") * 255.0
        image = image.astype(np.uint8)
        image_color = cv.cvtColor(image, cv.COLOR_GRAY2RGB)

        color = (0, 0, 255)
        if label[0] > 0:
            color = (0, 255, 0)

        cv.rectangle(image_color, box.numpy(), color, 2)

        plt.imshow(image_color)
        plt.axis("off")

from tensorflow import keras

def build_feature_extractor(inputs):
  #Input = keras.Input(shape = (800,800,3),name = 'input img')
  x = layers.Conv2D(64,3,padding ='same',input_shape=(input_size, input_size, 1), activation = 'relu')(inputs)
  x = layers.Conv2D(64,2,padding = 'same', activation ='relu')(x)
  output1 = layers.MaxPool2D(pool_size =(2,2),strides =(2,2),)(x)

  input2 = layers.Conv2D(128,3,padding ='same', activation ='relu')(output1)
  x = layers.Conv2D(128,3,padding ='same',activation ='relu')(input2)
  output2 = layers.MaxPool2D(pool_size = (2,2),strides =(2,2))(x)

  input3 = layers.Conv2D(256,3,padding ='same', activation ='relu')(output2)
  x = layers.Conv2D(256,3,padding ='same', activation ='relu')(input2)
  x = layers.Conv2D(256,3,padding ='same', activation ='relu')(x)
  output3 = layers.MaxPool2D(pool_size = (2,2),strides =(2,2))(x)

  input4 = layers.Conv2D(512,3,padding = 'same', activation = 'relu')(output3)
  x = layers.Conv2D(512,3,padding = 'same', activation = 'relu')(input4)
  x = layers.Conv2D(512,3,padding = 'same', activation = 'relu')(x)
  output4 = layers.MaxPool2D(pool_size = (2,2),strides =(2,2))(x)

  input5 = layers.Conv2D(512,3,padding ='same', activation = 'relu')(output4)
  x = layers.Conv2D(512,3,padding = 'same', activation = 'relu')(input5)
  x = layers.Conv2D(512,3,padding = 'same', activation = 'relu')(x)
  x = tf.keras.layers.Dropout(0.5)(x)
  x = layers.MaxPool2D(pool_size = (2,2),strides =(2,2),name = 'end')(x)
  return x

def build_model_adaptor(inputs):
  x = tf.keras.layers.Flatten()(inputs)
  x = tf.keras.layers.Dense(64,activation = 'relu')(x)
  return x

def build_classifier(inputs):
  return tf.keras.layers.Dense(CLASSES,activation = 'softmax',name = 'Classifier')(inputs)

def build_regressor(inputs):
  return tf.keras.layers.Dense(4,name = 'regressor')(inputs)


def build_model(inputs):
  feature_extractor = build_feature_extractor(inputs)

  model_adaptor = build_model_adaptor(feature_extractor)

  classifier = build_classifier(model_adaptor)

  regressor = build_regressor(model_adaptor)

  model = tf.keras.Model(inputs = inputs, outputs = [classifier,regressor])
  model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss = {'Classifier' : 'categorical_crossentropy', 'regressor' : 'mse' },
              metrics = {'Classifier' : 'accuracy', 'regressor' : 'mse' })

  return model



model = build_model(tf.keras.layers.Input(shape=(input_size, input_size, 1,)))

model.summary()

from tensorflow.keras.utils import plot_model

plot_model(model, show_shapes=True, show_layer_names=True)

EPOCHS = 20
history = model.fit(train_ds,
                  steps_per_epoch=(len(training_files) // BATCH_SIZE),
                  validation_data=validation_ds, validation_steps=1,
                  epochs=EPOCHS,
                  verbose=2)



plt.plot(history.history['Classifier_accuracy'])
plt.plot(history.history['val_Classifier_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

plt.plot(history.history['Classifier_loss'])
plt.plot(history.history['val_Classifier_loss'])
plt.title('Classification Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

plt.plot(history.history['regressor_loss'])
plt.plot(history.history['val_regressor_loss'])
plt.title('Bounding Box Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# adapted from: https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/
def intersection_over_union(boxA, boxB):
	xA = max(boxA[0], boxB[0])
	yA = max(boxA[1], boxB[1])
	xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
	yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])
	interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
	boxAArea = (boxA[2] + 1) * (boxA[3] + 1)
	boxBArea = (boxB[2] + 1) * (boxB[3] + 1)
	iou = interArea / float(boxAArea + boxBArea - interArea)
	return iou



def tune_test_ds(dataset):
    dataset = dataset.map(format_instance, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(1)
    dataset = dataset.repeat()
    return dataset

test_ds = tune_test_ds(raw_test_ds)

plt.figure(figsize=(20, 10))

test_list = list(test_ds.take(20).as_numpy_iterator())

print(len(test_list))

image, labels = test_list[0]

for i in range(len(test_list)):

    ax = plt.subplot(4, 10, i + 1)
    image, labels = test_list[i]

    predictions = model(image)

    predicted_box = predictions[1][0] * input_size
    predicted_box = tf.cast(predicted_box, tf.int32)

    predicted_label = predictions[0][0]

    image = image[0]

    actual_label = labels[0][0]
    actual_box = labels[1][0] * input_size
    actual_box = tf.cast(actual_box, tf.int32)

    image = image.astype("float") * 255.0
    image = image.astype(np.uint8)
    image_color = cv.cvtColor(image, cv.COLOR_GRAY2RGB)

    color = (255, 0, 0)
    # print box red if predicted and actual label do not match
    if (predicted_label[0] > 0.5 and actual_label[0] > 0) or (predicted_label[0] < 0.5 and actual_label[0] == 0):
        color = (0, 255, 0)

    cv.rectangle(image_color, predicted_box.numpy(), color, 2)
    cv.rectangle(image_color, actual_box.numpy(), (0, 0, 255), 2)

    IoU = intersection_over_union(predicted_box.numpy(), actual_box.numpy())

    plt.title("IoU:" + format(IoU, '.4f'))
    plt.imshow(image_color)
    plt.axis("off")